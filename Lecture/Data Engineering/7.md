# Chapiter 7

Data Exploration, investigate the data : just know it\
-> use graphic to understand the corolations, distributions\
-> show the missing value percentage

Bad data -> wrong decision

Incomplete
Inconstancy (wrong action, non delete propagation)

Ways to do data analytics :
- is the data structured/unstructured
- does the data lineage ?
- data get transformed ?
- fix data type
- having a document to map business value

|Technique|Description|
|-|-|
|Restricting|filtering : by a fix value|
|Sifting|like filtering but with multiple values : everything from $this_date to $today|
Grouping|spliting the data + applying agregation function + combining|

## Data Profiling :

````python
import pandas as df

df.info()
# Give some counts about the data
df.describe()
# Give the five numbers (med, mean, quartile, mod)
````

````
Note :
- Mod -> most occured data. It's the data with the greater frequency
````

````python
import pandas_profiling
# Can give some further statistics about the data set
# Can give some further statistics about the data attribut
````

## Combining and binning data 

### Definitions 
- Binning : tranform descritive values to quantitative
- Combine : many sources to one (concatenation) 

- Staging : intermated layer where everything is gather before loading

- Set operations : union (must have the same number of columns with the same types), interpolation, etc...

### Ways to do it

````python
# concat tables with shared columns
pd.concat([df1, df2])
# concat tables without merging columns
pd.concat([df1, df2], axis=1)
````

````python
# one to one with default commuon field
pd.merge(df1, df2)
# one to one with specitfied commuon field
pd.merge([df1, df2], on='A')
````

````python
# many to many 
pd.merge(df1, df2)
# warning : cross join so huged amout of repetition
# better to use cross reference table
````

````python
# Innner join
display('def6', 'df7', "pd.merge(df6,df7, how='inner')")
# Outer join
display('def6', 'df7', "pd.merge(df6,df7, how='outer')")
````
### Bins

````python
df_user_data_conbined['age_bin'] = pd.cut(x=df_ser_data_combined['age(D)'],
bins=[1,365,365*2,365*3,9999],
labels=['< 1 year',...]
````

- equal-width = distance
- equal-depth = frequency

also named as smoothing in time series, it's aimed to transform numerical to categorization

### Normalization / Scalling

- min-max scaling
- z-score standardisation

Depends on data distributions

````
standard deviations : Ã©cart-type
````

