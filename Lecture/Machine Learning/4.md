
## Ensemble learning

Baging vs. Boosting

Wisdom of crowd : 
- week classifier
- independant : following models are not aware of previous result

No free lunch ! 

Ensemble method : high probability result
Week classifier are used

## Random Forest

Without pruning ! All the tree have the same weight.

Boot Strap Agregation :
- sampling with replacement 
- sampling with no replacement

Average of the result

Compromise between complexity and rapidity -> ensemble model need a lot of compute methods

#### Baging -> average (agreagate), can be parallelise 

Bagging = Boot Strap Agregation using Sampling with replacement
Pasting = Boot Strap Agregation using Sampling without replacement

Training Set -> Bootstrap -> Predictor

Estimator = number of trees generated

#### Boosting -> sequential type learning

- AdaBoost (Adaptive Boosting)
- Gradient Booting -> can add a model if needed\
Parameters :
    - Number of trees
    - Depth of trees
    - Learning rate : deviation of error 
    - Subsampling : stockasting (SGD) take just a part of dataset
- XG Boost -> IMPORTANT ! (pour plus tard)
- Stacking -> multiple models outputs are used as input to another models

## Feature Importance

Using Information Game or Information On Gain Mesure or... there is a lot of others.
