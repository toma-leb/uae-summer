# Chapitre 3

## Naive Bayes Classifier

Machine Learning methods based on the posterior probability for classification.

Naive : we say that all the features are independant 

Iteration on binaries classification :
- All DataSet : Class A vs. the rest
- DataSet - Class A : Class B vs the rest 
- DataSet - Class A - Class B : Class C vs the rest 
- DataSet - Class A - Class B - Class C : Class D 

Posterior Probability : P(A|B) = P(B|A).P(A) / P(B)

For each object we will avaluate the posterior probablity and classifie the object in the classes with the max propability

No need to divide by P(B) because we are comparing values that would be divided by P(B) in all cases due to posterior probability function

## K-Mean

We have to choose the way we want to calculate the mean, it's depends on the data type :
- cosine &#8594; document
- euclidien &#8594; time series
- hamiltonien &#8594; ???

it's a <b>lazy learner</b> because it's doesn't generate a discrimative function

Voronoi : generate a figure arround object/classes where every point is the has the same closest object

If a new data falls into this figure, then it has the same classes as the nearest object

Elbow : comparing the sum distance metrics of the cluster 

## Decision Trees

If n sample of trees &#8594; random forest and we choose the majority one.

It's about binary and non-binary tree.

The <b>first-split</b> : the one that has the more information and give me the shorter tree

This algorithm is a rule based algorithm

I can have N features but only 1, 2, ..., n nodes : it depends on the data distribution

If we have to use a binary tree (faster) &#8594; needs to group values of non-binary features (try all variations and takes the better)

### Pruning

Pre-pruning &#8594; don't calculate unwanted branch

Post-pruning &#8594; calculate and remove unwanted branch

Pruning add errors : as always we have make a compromise

### Purity

Purity Measure vs. Un-purity Measure &#8594; entropy

The different type of decision tree are due to the purity or un-purity or pruning methods used.

### Information entropy

The highest it is the more pure the feature is.

The feature with the highest information entropy will be the first split.

### ID3

Information On Gain Mesure to find the entropy
