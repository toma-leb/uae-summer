# Chapitre 3

## Naive Bayes Classifier

Machine Learning methods based on the posterior probability for classification.

Naive : we say that all the features are independant 

Iteration on binaries classification :
- All DataSet : Class A vs. the rest
- DataSet - Class A : Class B vs the rest 
- DataSet - Class A - Class B : Class C vs the rest 
- DataSet - Class A - Class B - Class C : Class D 

Posterior Probability : P(A|B) = P(B|A).P(A) / P(B)

For each object we will avaluate the posterior probablity and classifie the object in the classes with the max propability

No need to divide by P(B) because we are comparing values that would be divided by P(B) in all cases due to posterior probability function

## K-Mean

We have to choose the way we want to calculate the mean, it's depends on the data type 

- cosine &#8594; document
- euclidien &#8594; time series
- hamiltonien &#8594; ???

it's a <b>lazy learner</b> because it's doesn't generate a discrimative function

Voronoi : generate a figure arround object/classes where every point is the has the same closest object

If a new data falls into this figure, then it has the same classes as the nearest object

Elbow : comparing the sum distance metrics of the cluster 

## Decision Trees

